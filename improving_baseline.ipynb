{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "from sklearn.preprocessing import LabelEncoder \n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import datetime\n",
    "import pickle\n",
    "from pprint import pprint\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import seaborn as sns\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#### User features ####\n",
    "id: user_id_hash 16264937\n",
    "user_recs\n",
    "user_clicks \n",
    "user_target_recs\n",
    "page_view_start_time\n",
    "#######################\n",
    "\n",
    "#### Item features ####\n",
    "id: target_id_hash 95082\n",
    "syndicator_id_hash 2809\n",
    "campaign_id_hash 28052\n",
    "target_item_taxonomy 66\n",
    "placement_id_hash 1629\n",
    "empiric_calibrated_recs\n",
    "empiric_clicks\n",
    "target_item_taxonomy 66\n",
    "#######################\n",
    "\n",
    "#### Context ####\n",
    "publisher_id_hash 3\n",
    "source_id_hash 264338\n",
    "source_item_type 5\n",
    "browser_platform 5\n",
    "country_code 406\n",
    "region 1965\n",
    "os_family  7\n",
    "day_of_week\n",
    "time_of_day\n",
    "gmt_offset 36\n",
    "#################\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Better classifier\n",
    "Build two tower model:\n",
    "* one tower for user embeddings\n",
    "* second tower for item embeddings\n",
    "* for each record the embedding will be the concat of numeric features and embedding of a lookup table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CategoricalFeature:\n",
    "    def __init__(self, name, vocabulary_size, embedding_size) -> None:\n",
    "        # + 1 for unknown\n",
    "        self.lookup_table = nn.Embedding(vocabulary_size + 1, embedding_size)\n",
    "        self.name = name\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "\n",
    "    def to_embeddings(self, idxs):\n",
    "        if not torch.is_tensor(idxs):\n",
    "            idxs = torch.tensor(idxs)\n",
    "        return self.lookup_table(idxs)\n",
    "\n",
    "class FeaturesEncoder:\n",
    "    @staticmethod\n",
    "    def get_instance(name: str, features_numeric, features_categoric = None):\n",
    "        if features_categoric is None:\n",
    "            filepath = os.path.join('.', 'lookup_tables', f'{name}-{\"-\".join(features_numeric)}.pickle')\n",
    "        else:\n",
    "            filepath = os.path.join('.', 'lookup_tables', f'{name}-{\"-\".join(features_numeric)}-{\"-\".join(features_categoric)}.pickle')\n",
    "        if os.path.exists(filepath):\n",
    "            with open(filepath, 'rb') as handle:\n",
    "                encoder = pickle.load(handle)\n",
    "            print(f'{name.capitalize()} loaded from {filepath}')\n",
    "        else:\n",
    "            encoder = FeaturesEncoder(features_numeric, features_categoric)\n",
    "            with open(filepath, 'wb') as handle:\n",
    "                pickle.dump(encoder, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            print(f'{name.capitalize()} initialized and saved to {filepath}')\n",
    "        return encoder\n",
    "    \n",
    "    def __init__(self, features_numeric, features_categoric ) -> None:\n",
    "        self.features_numeric = features_numeric\n",
    "        self.features_categoric = [] if features_categoric is None else list(features_categoric.keys())\n",
    "        self.features_categoric_lookup_table = dict() if features_categoric is None else dict([\n",
    "            (feature_name, CategoricalFeature(\n",
    "                name=feature_name,\n",
    "                vocabulary_size=vocabulary_size,\n",
    "                embedding_size=embedding_size,\n",
    "            ))\n",
    "            for feature_name, (vocabulary_size, embedding_size) in features_categoric.items()\n",
    "        ])\n",
    "\n",
    "    def to_emebeddings(self, df: pd.DataFrame):\n",
    "        numeric_features = df[self.features_numeric].to_numpy()\n",
    "        df.drop(columns=self.features_numeric, inplace=True)\n",
    "        numeric_features = torch.from_numpy(numeric_features)\n",
    "        categoric_features = []\n",
    "        for feature in self.features_categoric:\n",
    "            encoder = self.features_categoric_lookup_table[feature]\n",
    "            categoric_feature = df[feature].to_numpy()\n",
    "            df.drop(columns=[feature], inplace=True)\n",
    "            categoric_feature = torch.from_numpy(categoric_feature)\n",
    "            feature_embedding = encoder.to_embeddings(categoric_feature)\n",
    "            categoric_features.append(feature_embedding)\n",
    "        # concat (numeric, ...categoric)\n",
    "        return torch.cat((numeric_features, *categoric_features), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainTestMaker:\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        data = TrainTestMaker.read_all_data()\n",
    "        self.data: pd.DataFrame = data\n",
    "        user_features_numeric, item_features_numeric, item_features_categoric = TrainTestMaker.get_features_defs(data)\n",
    "        self.user_encoder = FeaturesEncoder.get_instance('user', user_features_numeric)\n",
    "        self.item_encoder = FeaturesEncoder.get_instance('item', item_features_numeric, item_features_categoric)\n",
    "\n",
    "    @staticmethod\n",
    "    def read_all_data() -> pd.DataFrame:\n",
    "        paths = []\n",
    "        dirs = os.listdir('normalized_train_data')\n",
    "        for dir in dirs:\n",
    "            filenames = os.listdir(f'normalized_train_data/{dir}')\n",
    "            paths += [f'normalized_train_data/{dir}/{filename}' for filename in filenames]\n",
    "        paths = sorted(paths)\n",
    "        dfs = []\n",
    "        for filepath in tqdm(paths, desc='Loading data'):\n",
    "            df = pd.read_csv(filepath)\n",
    "            df.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "            dfs.append(df)\n",
    "        dfs = pd.concat(dfs)\n",
    "        dfs.reset_index(drop=True, inplace=True)\n",
    "        return dfs\n",
    "\n",
    "    @staticmethod\n",
    "    def load_encoders():\n",
    "        features = [\n",
    "            'user_id_hash',\n",
    "            'target_id_hash',\n",
    "            'syndicator_id_hash',\n",
    "            'campaign_id_hash',\n",
    "            'target_item_taxonomy',\n",
    "            'placement_id_hash',\n",
    "            'publisher_id_hash',\n",
    "            'source_id_hash',\n",
    "            'source_item_type',\n",
    "            'browser_platform',\n",
    "            'country_code',\n",
    "            'region',\n",
    "        ]\n",
    "        column_encoders = dict()\n",
    "        for feature in tqdm(features, desc='Loading encoders'):\n",
    "            with open(f'./label_encoders/{feature}.pickle', 'rb') as handle:\n",
    "                encoder = pickle.load(handle)\n",
    "            column_encoders[feature] = encoder\n",
    "        return column_encoders\n",
    "\n",
    "    @staticmethod\n",
    "    def get_features_defs(data):\n",
    "        user_features_numeric = [\n",
    "            'page_view_start_time',\n",
    "            'user_recs',\n",
    "            'user_clicks', \n",
    "            'user_target_recs',\n",
    "        ]\n",
    "\n",
    "        item_features_numeric = [\n",
    "            'page_view_start_time',\n",
    "            'empiric_calibrated_recs',\n",
    "            'empiric_clicks',\n",
    "        ]\n",
    "\n",
    "        item_features_categoric = {\n",
    "            # (vocabulary size, embedding size)\n",
    "            'syndicator_id_hash': (data['syndicator_id_hash'].nunique(), 32),\n",
    "            'campaign_id_hash': (data['campaign_id_hash'].nunique(), 32),\n",
    "            'placement_id_hash': (data['placement_id_hash'].nunique(), 32),\n",
    "            'target_item_taxonomy': (data['target_item_taxonomy'].nunique(), 8),\n",
    "        }\n",
    "        return user_features_numeric, item_features_numeric, item_features_categoric\n",
    "\n",
    "\n",
    "    def _get_split(self):\n",
    "        max_timestamp = self.data['page_view_start_time'].max()\n",
    "        three_days_ago = (datetime.datetime.fromtimestamp(max_timestamp/1000) - datetime.timedelta(days=3)).replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "        three_days_ago = int(datetime.datetime.timestamp(three_days_ago) * 1000)\n",
    "        return three_days_ago\n",
    "\n",
    "    def get_trainset_df(self):\n",
    "        threshold = self._get_split()\n",
    "        train_set = self.data[self.data['page_view_start_time'] < threshold]\n",
    "        train_set.reset_index(drop=True, inplace=True)\n",
    "        return train_set\n",
    "\n",
    "    def df_to_embeddings(self, df):\n",
    "        batch_size = 1_000_000\n",
    "        user_embeddings = []\n",
    "        item_embeddings = []\n",
    "        for i in range(0, len(df.index), batch_size):\n",
    "            batch = df.iloc[i:i + batch_size]\n",
    "            user_features_df = batch[self.user_encoder.features_numeric + self.user_encoder.features_categoric].copy(deep=True)\n",
    "            del batch\n",
    "            batch_user_embeddings = self.user_encoder.to_emebeddings(user_features_df)\n",
    "            user_embeddings.append(batch_user_embeddings)\n",
    "            del user_features_df\n",
    "            batch = df.iloc[i:i + batch_size]\n",
    "            item_features_df = batch[self.item_encoder.features_numeric + self.item_encoder.features_categoric].copy(deep=True)\n",
    "            del batch\n",
    "            batch_item_embeddings = self.item_encoder.to_emebeddings(item_features_df)\n",
    "            item_embeddings.append(batch_item_embeddings)\n",
    "            del item_features_df\n",
    "        user_embeddings_concatenated = torch.cat(user_embeddings)\n",
    "        del user_embeddings\n",
    "        item_embeddings_concatenated = torch.cat(item_embeddings)\n",
    "        del item_embeddings\n",
    "        return {\n",
    "            'user': user_embeddings_concatenated,\n",
    "            'item': item_embeddings_concatenated,\n",
    "            'label': torch.from_numpy(df['is_click'].to_numpy()),\n",
    "        }\n",
    "    \n",
    "    def get_test_set_df(self):\n",
    "        threshold = self._get_split()\n",
    "        train_set = self.data[self.data['page_view_start_time'] < threshold]\n",
    "        last_three_days = self.data[self.data['page_view_start_time'] >= threshold]\n",
    "        not_cold_users_mask = last_three_days['user_id_hash'].isin(train_set['user_id_hash'])\n",
    "        test_set_hot_users = last_three_days[not_cold_users_mask]\n",
    "        test_set_cold_users = last_three_days[~not_cold_users_mask]\n",
    "        test_set_hot_users.reset_index(drop=True, inplace=True)\n",
    "        test_set_cold_users.reset_index(drop=True, inplace=True)\n",
    "        return test_set_hot_users, test_set_cold_users\n",
    "\n",
    "    def get_train_set(self, max_idx = None):\n",
    "        print('Extracting Train set')\n",
    "        train_set_df = self.get_trainset_df()\n",
    "        print('Extraction completed')\n",
    "        if max_idx is not None:\n",
    "            train_set_df = train_set_df.iloc[: max_idx].copy()\n",
    "        print('Converting to embeddings')\n",
    "        train_set = self.df_to_embeddings(train_set_df)\n",
    "        print('Convertion completed')\n",
    "        return train_set\n",
    "    \n",
    "    def get_test_set(self):\n",
    "        test_set_hot_users_df, test_set_cold_users_df = self.get_test_set_df()\n",
    "        test_set_hot_users = self.df_to_embeddings(test_set_hot_users_df)\n",
    "        del test_set_hot_users_df\n",
    "        test_set_cold_users = self.df_to_embeddings(test_set_cold_users_df)\n",
    "        del test_set_cold_users_df\n",
    "        return test_set_hot_users, test_set_cold_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fb61073ac344866955d6968758ccda0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading data:   0%|          | 0/69 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User loaded from .\\lookup_tables\\user-page_view_start_time-user_recs-user_clicks-user_target_recs.pickle\n",
      "Item loaded from .\\lookup_tables\\item-page_view_start_time-empiric_calibrated_recs-empiric_clicks-syndicator_id_hash-campaign_id_hash-placement_id_hash-target_item_taxonomy.pickle\n"
     ]
    }
   ],
   "source": [
    "data_maker = TrainTestMaker()\n",
    "# train_set = data_maker.get_train_set(3_000_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        self.user_tower = nn.Sequential(\n",
    "            nn.Linear(4, 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4, 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4, 4),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.item_tower = nn.Sequential(\n",
    "            nn.Linear(107, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        users, items = x\n",
    "        users = self.user_tower(users)\n",
    "        items = self.item_tower(items)\n",
    "        users = nn.functional.pad(users, (0,60), value=1)\n",
    "        aggregation = users * items\n",
    "        return self.classifier(aggregation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mTorchDataset\u001b[39;00m(\u001b[43mDataset\u001b[49m):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idxs, data):\n\u001b[0;32m      3\u001b[0m         \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Dataset' is not defined"
     ]
    }
   ],
   "source": [
    "class TorchDataset(Dataset):\n",
    "    def __init__(self, idxs, data):\n",
    "        super().__init__()\n",
    "        self.idxs = idxs\n",
    "        self.data = data\n",
    "        self.size = idxs.shape[0]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        users_main_data = self.data['user']\n",
    "        items_main_data = self.data['item']\n",
    "        y_main_data = self.data['label']\n",
    "        \n",
    "        real_idxs = self.idxs[idx]\n",
    "        \n",
    "        users = users_main_data[real_idxs]\n",
    "        items = items_main_data[real_idxs]\n",
    "        y = y_main_data[real_idxs]\n",
    "        \n",
    "        return users, items, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory .\\models\\torch-models-2024-02-16T11-26-39\n",
      "Extracting Train set\n",
      "Extraction completed\n",
      "Converting to embeddings\n",
      "Convertion completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Mark\\anaconda3\\envs\\research\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:557: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 8 (`cpuset` is not taken into account), which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5765df09721f419185b6e93ac769fcce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs loop:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "class EarlyStopper:\n",
    "    def __init__(self, patience=1, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_validation_loss = float('inf')\n",
    "\n",
    "    def early_stop(self, validation_loss):\n",
    "        if validation_loss < self.min_validation_loss:\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "        elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "def split_given_size(a, size):\n",
    "    return np.split(a, np.arange(size,len(a),size))\n",
    "\n",
    "class Training:\n",
    "\n",
    "    def __init__(self, data_maker: TrainTestMaker, models_dir, data_size=None) -> None:\n",
    "        self.data_maker = data_maker\n",
    "        data = data_maker.get_train_set(data_size)\n",
    "        model = Model()\n",
    "        # model = torch.compile(model)\n",
    "        model.eval()\n",
    "        self.model = model\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        # self.device = torch.device(\"cpu\")\n",
    "        self.validation_percent = 0.05\n",
    "        self.epochs = 3\n",
    "\n",
    "        classes, class_occurances = data['label'].unique(return_counts=True)\n",
    "        max_class_occurances = class_occurances.max()\n",
    "        weights = (max_class_occurances / class_occurances).to(self.device)\n",
    "        self._ce_weights = weights\n",
    "\n",
    "        self.criterion = torch.nn.CrossEntropyLoss(weight=weights, label_smoothing=0.15)\n",
    "        self.optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "        self._training_objects = dict()\n",
    "        self.data = data\n",
    "        self.models_dir = models_dir\n",
    "\n",
    "    def _save(self, models_dir, epoch=None):\n",
    "        if not os.path.isdir(models_dir):\n",
    "            os.mkdir(models_dir)\n",
    "        name = 'model-final.pt' if epoch is None else f'model-epoch-{epoch}.pt'\n",
    "        torch.save(self.model, os.path.join(models_dir, name))\n",
    "\n",
    "\n",
    "    def _build_loaders(self):\n",
    "        users = self.data['user']\n",
    "\n",
    "        permutation = np.random.permutation(len(users))\n",
    "        validation_size = int(len(users) * self.validation_percent)\n",
    "        validation_idxs = permutation[:validation_size]\n",
    "        train_idxs = permutation[validation_size:]\n",
    "        train_size = len(train_idxs)\n",
    "        \n",
    "        train_dataset = TorchDataset(train_idxs, self.data)\n",
    "        validation_dataset = TorchDataset(validation_idxs, self.data)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=128, num_workers=8, shuffle=True)\n",
    "        val_loader = DataLoader(validation_dataset, batch_size=128, num_workers=10, shuffle=False)\n",
    "        return train_loader, train_size, val_loader, validation_size\n",
    "\n",
    "    def _batch_train(self, data, stage):\n",
    "        users, items, labels = data\n",
    "        # mode to device/cuda\n",
    "        users, items, labels = users.to(self.device), items.to(self.device), labels.to(self.device)\n",
    "        # zero the parameter gradients\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        with torch.set_grad_enabled(stage == 'train'):\n",
    "            # logits: The raw predictions from the last layer            \n",
    "            logits = self.model((users, items))\n",
    "            _, preds = torch.max(logits, 1)\n",
    "            # return preds, labels\n",
    "            # using CE criterion, thus input is the logits\n",
    "            loss = self.criterion(logits, labels)\n",
    "            if stage == 'train':\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "        # statistics\n",
    "        running_loss = loss.item() * users.size(0)\n",
    "        running_corrects = torch.sum(preds == labels.argmax())\n",
    "        return running_loss, running_corrects\n",
    "\n",
    "    def _train(self, train_loader, train_size, val_loader, validation_size, models_dir):\n",
    "        metrics = {\n",
    "            'epochs': [],\n",
    "            'loss': {\n",
    "                'train': [],\n",
    "                'validation': [],\n",
    "            },\n",
    "            'accuracy': {\n",
    "                'train': [],\n",
    "                'validation': [],\n",
    "            }\n",
    "        }\n",
    "        print('Training the model')\n",
    "        early_stopper = EarlyStopper(patience=3, min_delta=0.1)\n",
    "        for epoch in tqdm(np.arange(self.epochs), desc=f'Epochs loop'):\n",
    "            for stage in ['train', 'validation']:\n",
    "                loss = self._epoch_train(stage, metrics, train_loader, train_size, val_loader, validation_size)\n",
    "                if stage == 'validation':\n",
    "                    validation_loss = loss\n",
    "            self._save(models_dir, epoch)\n",
    "            metrics['epochs'].append(epoch)\n",
    "            if early_stopper.early_stop(validation_loss):\n",
    "                print(f'Early stopping on epoch {epoch}')\n",
    "                break\n",
    "        print('Finished Training')\n",
    "        self._save(models_dir)\n",
    "        if not os.path.isdir(os.path.join('.', 'objects')):\n",
    "            os.mkdir(os.path.join('.', 'objects'))\n",
    "        with open(os.path.join(models_dir, 'training-metrics.pickle'), 'wb') as handle:\n",
    "            pickle.dump(metrics, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        return metrics\n",
    "\n",
    "\n",
    "    def _epoch_train(self, stage, metrics, train_loader, train_size, val_loader, validation_size):\n",
    "        epoch_loss = 0\n",
    "        epoch_accuracy = 0\n",
    "        if stage == 'train':\n",
    "            self.model.train()\n",
    "            loader = train_loader\n",
    "            dataset_size = train_size\n",
    "        else:\n",
    "            self.model.eval()\n",
    "            loader = val_loader\n",
    "            dataset_size = validation_size\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        for i, data in tqdm(enumerate(loader, 0), total=len(loader), desc=f'Stage {stage}: iterating over batches'):\n",
    "            # return _batch_train(data, stage)\n",
    "            current_loss, current_corrects = self._batch_train(data, stage)\n",
    "            running_loss += current_loss\n",
    "            running_corrects += current_corrects\n",
    "            break\n",
    "        epoch_loss = running_loss / dataset_size\n",
    "        epoch_accuracy = running_corrects.double() / dataset_size\n",
    "        # accuracy is still tensor\n",
    "        metrics['accuracy'][stage].append(epoch_accuracy.item())\n",
    "        # loss in python native type number\n",
    "        metrics['loss'][stage].append(epoch_loss)\n",
    "        return epoch_loss\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = split_given_size(X, 64)\n",
    "        with torch.no_grad():\n",
    "            self.model.eval()\n",
    "            outputs = []\n",
    "            probabilities_acc = []\n",
    "            for chunk in tqdm(X, desc='Prediction'):\n",
    "                inputs = torch.from_numpy(np.moveaxis(chunk, -1, 1)).float().to(self.device)\n",
    "                logits = self.model(inputs)\n",
    "                logits = self.model.softmax(logits)\n",
    "                probabilities = logits.cpu()\n",
    "                predictions = logits.argmax(dim=1).cpu()\n",
    "                outputs.append(predictions)\n",
    "                probabilities_acc.append(probabilities)\n",
    "            return torch.cat(outputs).numpy(), torch.cat(probabilities_acc).numpy()\n",
    "\n",
    "    def show_metrics(self, metrics, dirpath):\n",
    "        loss_df = pd.DataFrame(index=metrics['epochs'])\n",
    "        loss_df['train'] = metrics['loss']['train']\n",
    "        loss_df['validation'] = metrics['loss']['validation']\n",
    "        accuracy_df = pd.DataFrame(index=metrics['epochs'])\n",
    "        accuracy_df['train'] = metrics['accuracy']['train']\n",
    "        accuracy_df['validation'] = metrics['accuracy']['validation']\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 1, figsize=(10,10))\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        sns.lineplot(data=loss_df, ax=axes[0])\n",
    "        axes[0].grid()\n",
    "        axes[0].set_ylabel('Loss')\n",
    "        axes[0].set_xlabel('Epochs')\n",
    "\n",
    "        sns.lineplot(data=accuracy_df, ax=axes[1])\n",
    "        axes[1].grid()\n",
    "        axes[1].set_ylabel('Accuracy')\n",
    "        axes[1].set_xlabel('Epochs')\n",
    "        fig.tight_layout()\n",
    "        fig.savefig(os.path.join(dirpath, 'loss-accuracy-plots-during-training.png'))\n",
    "        plt.close(fig)\n",
    "\n",
    "    def run(self):\n",
    "        train_loader, train_size, val_loader, validation_size = self._build_loaders()\n",
    "        model = self.model.to(self.device)\n",
    "        res = self._train(train_loader, train_size, val_loader, validation_size, self.models_dir)\n",
    "        # with open(os.path.join(self.models_dir, 'training-metrics.pickle'), 'rb') as handle:\n",
    "        #     metrics = pickle.load(handle)\n",
    "        # self.show_metrics(metrics, self.models_dir)\n",
    "        # del self.data\n",
    "        # # collect garbage\n",
    "        # gc.collect()\n",
    "        # test_set_hot_users, test_set_cold_users = data_maker.get_test_set()\n",
    "        # predictions, probabilities = self.predict(test_set_hot_users)\n",
    "        # report = pd.DataFrame()\n",
    "        \n",
    "        # report['ground-truth'] = test_set_hot_users['label'].numpy().astype('int')\n",
    "        # report['predictions'] = predictions\n",
    "        # report['prediction-proba-0'] = probabilities[:, 0]\n",
    "        # report['prediction-proba-1'] = probabilities[:, 1]\n",
    "        # report.to_csv(os.path.join(self.models_dir, 'hot-users-predictions.csv'), index=False)\n",
    "\n",
    "        # del test_set_hot_users\n",
    "\n",
    "        # predictions, probabilities = self.predict(test_set_cold_users)\n",
    "        # report = pd.DataFrame()\n",
    "        # report['ground-truth'] = test_set_cold_users['label'].numpy().astype('int')\n",
    "        # report['predictions'] = predictions\n",
    "        # report['prediction-proba-0'] = probabilities[:, 0]\n",
    "        # report['prediction-proba-1'] = probabilities[:, 1]\n",
    "        # report.to_csv(os.path.join(self.models_dir, 'cold-users-predictions.csv'), index=False)\n",
    "        # del test_set_cold_users\n",
    "\n",
    "\n",
    "def run():\n",
    "    current_datetime = datetime.datetime.now()\n",
    "    date_str = current_datetime.strftime(\"%Y-%m-%d\")\n",
    "    time_str = current_datetime.strftime(\"%H-%M-%S\")\n",
    "    models_dir = os.path.join('.', 'models', f\"torch-models-{date_str}T{time_str}\")\n",
    "    print(f'Directory {models_dir}')\n",
    "    if not os.path.isdir(models_dir):\n",
    "        os.mkdir(models_dir)\n",
    "    train_manager = Training(\n",
    "        data_maker,\n",
    "        models_dir=models_dir,\n",
    "        data_size=3_000_000,\n",
    "    )\n",
    "    train_manager.run()\n",
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
