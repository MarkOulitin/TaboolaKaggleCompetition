{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "from sklearn.preprocessing import LabelEncoder \n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import datetime\n",
    "import pickle\n",
    "from pprint import pprint\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import seaborn as sns\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#### User features ####\\nid: user_id_hash 16264937\\nuser_recs\\nuser_clicks \\nuser_target_recs\\npage_view_start_time\\n#######################\\n\\n#### Item features ####\\nid: target_id_hash 95082\\nsyndicator_id_hash 2809\\ncampaign_id_hash 28052\\ntarget_item_taxonomy 66\\nplacement_id_hash 1629\\nempiric_calibrated_recs\\nempiric_clicks\\ntarget_item_taxonomy 66\\n#######################\\n\\n#### Context ####\\npublisher_id_hash 3\\nsource_id_hash 264338\\nsource_item_type 5\\nbrowser_platform 5\\ncountry_code 406\\nregion 1965\\nos_family  7\\nday_of_week\\ntime_of_day\\ngmt_offset 36\\n#################\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "#### User features ####\n",
    "id: user_id_hash 16264937\n",
    "user_recs\n",
    "user_clicks \n",
    "user_target_recs\n",
    "page_view_start_time\n",
    "#######################\n",
    "\n",
    "#### Item features ####\n",
    "id: target_id_hash 95082\n",
    "syndicator_id_hash 2809\n",
    "campaign_id_hash 28052\n",
    "target_item_taxonomy 66\n",
    "placement_id_hash 1629\n",
    "empiric_calibrated_recs\n",
    "empiric_clicks\n",
    "target_item_taxonomy 66\n",
    "#######################\n",
    "\n",
    "#### Context ####\n",
    "publisher_id_hash 3\n",
    "source_id_hash 264338\n",
    "source_item_type 5\n",
    "browser_platform 5\n",
    "country_code 406\n",
    "region 1965\n",
    "os_family  7\n",
    "day_of_week\n",
    "time_of_day\n",
    "gmt_offset 36\n",
    "#################\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Better classifier\n",
    "Build two tower model:\n",
    "* one tower for user embeddings\n",
    "* second tower for item embeddings\n",
    "* for each record the embedding will be the concat of numeric features and embedding of a lookup table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name\n",
      "NVIDIA GeForce GTX 1080 Ti\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi --query-gpu=gpu_name --format=csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CategoricalFeature:\n",
    "    def __init__(self, name, vocabulary_size, embedding_size) -> None:\n",
    "        # + 1 for unknown\n",
    "        self.lookup_table = nn.Embedding(vocabulary_size + 1, embedding_size)\n",
    "        self.name = name\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "\n",
    "    def to_embeddings(self, idxs):\n",
    "        if not torch.is_tensor(idxs):\n",
    "            idxs = torch.tensor(idxs)\n",
    "        with torch.no_grad():\n",
    "            embeddings = self.lookup_table(idxs)\n",
    "        return embeddings\n",
    "\n",
    "class FeaturesEncoder:\n",
    "    @staticmethod\n",
    "    def get_instance(name: str, features_numeric, features_categoric = None):\n",
    "        if features_categoric is None:\n",
    "            filepath = os.path.join('.', 'lookup_tables', f'{name}-{\"-\".join(features_numeric)}.pickle')\n",
    "        else:\n",
    "            filepath = os.path.join('.', 'lookup_tables', f'{name}-{\"-\".join(features_numeric)}-{\"-\".join(features_categoric)}.pickle')\n",
    "        if os.path.exists(filepath):\n",
    "            with open(filepath, 'rb') as handle:\n",
    "                encoder = pickle.load(handle)\n",
    "            print(f'{name.capitalize()} loaded from {filepath}')\n",
    "        else:\n",
    "            encoder = FeaturesEncoder(features_numeric, features_categoric)\n",
    "            with open(filepath, 'wb') as handle:\n",
    "                pickle.dump(encoder, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            print(f'{name.capitalize()} initialized and saved to {filepath}')\n",
    "        return encoder\n",
    "    \n",
    "    def __init__(self, features_numeric, features_categoric ) -> None:\n",
    "        self.features_numeric = features_numeric\n",
    "        self.features_categoric = [] if features_categoric is None else list(features_categoric.keys())\n",
    "        self.features_categoric_lookup_table = dict() if features_categoric is None else dict([\n",
    "            (feature_name, CategoricalFeature(\n",
    "                name=feature_name,\n",
    "                vocabulary_size=vocabulary_size,\n",
    "                embedding_size=embedding_size,\n",
    "            ))\n",
    "            for feature_name, (vocabulary_size, embedding_size) in features_categoric.items()\n",
    "        ])\n",
    "\n",
    "    def to_emebeddings(self, df: pd.DataFrame):\n",
    "        numeric_features = df[self.features_numeric].to_numpy()\n",
    "        df.drop(columns=self.features_numeric, inplace=True)\n",
    "        numeric_features = torch.from_numpy(numeric_features).float()\n",
    "        categoric_features = []\n",
    "        for feature in self.features_categoric:\n",
    "            encoder = self.features_categoric_lookup_table[feature]\n",
    "            categoric_feature = df[feature].to_numpy()\n",
    "            df.drop(columns=[feature], inplace=True)\n",
    "            categoric_feature = torch.from_numpy(categoric_feature)\n",
    "            feature_embedding = encoder.to_embeddings(categoric_feature)\n",
    "            categoric_features.append(feature_embedding)\n",
    "        # concat (numeric, ...categoric)\n",
    "        return torch.cat((numeric_features, *categoric_features), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainTestMaker:\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        data = TrainTestMaker.read_all_data()\n",
    "        self.data: pd.DataFrame = data\n",
    "        user_features_numeric, item_features_numeric, item_features_categoric = TrainTestMaker.get_features_defs(data)\n",
    "        self.user_encoder = FeaturesEncoder.get_instance('user', user_features_numeric)\n",
    "        self.item_encoder = FeaturesEncoder.get_instance('item', item_features_numeric, item_features_categoric)\n",
    "\n",
    "    @staticmethod\n",
    "    def read_all_data() -> pd.DataFrame:\n",
    "        paths = []\n",
    "        dirs = os.listdir('normalized_train_data')\n",
    "        for dir in dirs:\n",
    "            filenames = os.listdir(f'normalized_train_data/{dir}')\n",
    "            paths += [f'normalized_train_data/{dir}/{filename}' for filename in filenames]\n",
    "        paths = sorted(paths)\n",
    "        dfs = []\n",
    "        for filepath in tqdm(paths, desc='Loading data'):\n",
    "            df = pd.read_csv(filepath)\n",
    "            df.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "            dfs.append(df)\n",
    "        dfs = pd.concat(dfs)\n",
    "        dfs.reset_index(drop=True, inplace=True)\n",
    "        return dfs\n",
    "\n",
    "    @staticmethod\n",
    "    def load_encoders():\n",
    "        features = [\n",
    "            'user_id_hash',\n",
    "            'target_id_hash',\n",
    "            'syndicator_id_hash',\n",
    "            'campaign_id_hash',\n",
    "            'target_item_taxonomy',\n",
    "            'placement_id_hash',\n",
    "            'publisher_id_hash',\n",
    "            'source_id_hash',\n",
    "            'source_item_type',\n",
    "            'browser_platform',\n",
    "            'country_code',\n",
    "            'region',\n",
    "        ]\n",
    "        column_encoders = dict()\n",
    "        for feature in tqdm(features, desc='Loading encoders'):\n",
    "            with open(f'./label_encoders/{feature}.pickle', 'rb') as handle:\n",
    "                encoder = pickle.load(handle)\n",
    "            column_encoders[feature] = encoder\n",
    "        return column_encoders\n",
    "\n",
    "    @staticmethod\n",
    "    def get_features_defs(data):\n",
    "        user_features_numeric = [\n",
    "            'page_view_start_time',\n",
    "            'user_recs',\n",
    "            'user_clicks', \n",
    "            'user_target_recs',\n",
    "        ]\n",
    "\n",
    "        item_features_numeric = [\n",
    "            'page_view_start_time',\n",
    "            'empiric_calibrated_recs',\n",
    "            'empiric_clicks',\n",
    "        ]\n",
    "\n",
    "        item_features_categoric = {\n",
    "            # (vocabulary size, embedding size)\n",
    "            'syndicator_id_hash': (data['syndicator_id_hash'].nunique(), 32),\n",
    "            'campaign_id_hash': (data['campaign_id_hash'].nunique(), 32),\n",
    "            'placement_id_hash': (data['placement_id_hash'].nunique(), 32),\n",
    "            'target_item_taxonomy': (data['target_item_taxonomy'].nunique(), 8),\n",
    "        }\n",
    "        return user_features_numeric, item_features_numeric, item_features_categoric\n",
    "\n",
    "\n",
    "    def _get_split(self):\n",
    "        max_timestamp = self.data['page_view_start_time'].max()\n",
    "        three_days_ago = (datetime.datetime.fromtimestamp(max_timestamp/1000) - datetime.timedelta(days=3)).replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "        three_days_ago = int(datetime.datetime.timestamp(three_days_ago) * 1000)\n",
    "        return three_days_ago\n",
    "\n",
    "    def get_trainset_df(self):\n",
    "        threshold = self._get_split()\n",
    "        train_set = self.data[self.data['page_view_start_time'] < threshold]\n",
    "        train_set.reset_index(drop=True, inplace=True)\n",
    "        return train_set\n",
    "\n",
    "    def df_to_embeddings(self, df):\n",
    "        batch_size = 1_000_000\n",
    "        user_embeddings = []\n",
    "        item_embeddings = []\n",
    "        for i in range(0, len(df.index), batch_size):\n",
    "            batch = df.iloc[i:i + batch_size]\n",
    "            user_features_df = batch[self.user_encoder.features_numeric + self.user_encoder.features_categoric].copy(deep=True)\n",
    "            del batch\n",
    "            batch_user_embeddings = self.user_encoder.to_emebeddings(user_features_df)\n",
    "            user_embeddings.append(batch_user_embeddings)\n",
    "            del user_features_df\n",
    "            batch = df.iloc[i:i + batch_size]\n",
    "            item_features_df = batch[self.item_encoder.features_numeric + self.item_encoder.features_categoric].copy(deep=True)\n",
    "            del batch\n",
    "            batch_item_embeddings = self.item_encoder.to_emebeddings(item_features_df)\n",
    "            item_embeddings.append(batch_item_embeddings)\n",
    "            del item_features_df\n",
    "        user_embeddings_concatenated = torch.cat(user_embeddings)\n",
    "        del user_embeddings\n",
    "        item_embeddings_concatenated = torch.cat(item_embeddings)\n",
    "        del item_embeddings\n",
    "        labels = df['is_click'].to_numpy().astype('int')\n",
    "        labels_one_hot = np.zeros((labels.shape[0], 2))\n",
    "        labels_one_hot[np.arange(labels.shape[0]), labels] = 1\n",
    "        del labels\n",
    "        return {\n",
    "            'user': user_embeddings_concatenated,\n",
    "            'item': item_embeddings_concatenated,\n",
    "            'label': torch.from_numpy(labels_one_hot).float(),\n",
    "        }\n",
    "    \n",
    "    def get_test_set_df(self):\n",
    "        threshold = self._get_split()\n",
    "        train_set = self.data[self.data['page_view_start_time'] < threshold]\n",
    "        last_three_days = self.data[self.data['page_view_start_time'] >= threshold]\n",
    "        not_cold_users_mask = last_three_days['user_id_hash'].isin(train_set['user_id_hash'])\n",
    "        test_set_hot_users = last_three_days[not_cold_users_mask]\n",
    "        test_set_cold_users = last_three_days[~not_cold_users_mask]\n",
    "        test_set_hot_users.reset_index(drop=True, inplace=True)\n",
    "        test_set_cold_users.reset_index(drop=True, inplace=True)\n",
    "        return test_set_hot_users, test_set_cold_users\n",
    "\n",
    "    def get_train_set(self, max_idx = None):\n",
    "        print('Extracting Train set')\n",
    "        train_set_df = self.get_trainset_df()\n",
    "        print('Extraction completed')\n",
    "        if max_idx is not None:\n",
    "            train_set_df = train_set_df.iloc[: max_idx].copy()\n",
    "        print('Converting to embeddings')\n",
    "        train_set = self.df_to_embeddings(train_set_df)\n",
    "        print('Convertion completed')\n",
    "        return train_set\n",
    "    \n",
    "    def get_test_set(self):\n",
    "        test_set_hot_users_df, test_set_cold_users_df = self.get_test_set_df()\n",
    "        test_set_hot_users = self.df_to_embeddings(test_set_hot_users_df)\n",
    "        del test_set_hot_users_df\n",
    "        test_set_cold_users = self.df_to_embeddings(test_set_cold_users_df)\n",
    "        del test_set_cold_users_df\n",
    "        return test_set_hot_users, test_set_cold_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dec71f5deeb84c4498dd74d47e112796",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading data:   0%|          | 0/69 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User loaded from ./lookup_tables/user-page_view_start_time-user_recs-user_clicks-user_target_recs.pickle\n",
      "Item loaded from ./lookup_tables/item-page_view_start_time-empiric_calibrated_recs-empiric_clicks-syndicator_id_hash-campaign_id_hash-placement_id_hash-target_item_taxonomy.pickle\n"
     ]
    }
   ],
   "source": [
    "data_maker = TrainTestMaker()\n",
    "# train_set = data_maker.get_train_set(3_000_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        self.user_tower = nn.Sequential(\n",
    "            nn.Linear(4, 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4, 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4, 4),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.item_tower = nn.Sequential(\n",
    "            nn.Linear(107, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        users, items = x\n",
    "        users = self.user_tower(users)\n",
    "        items = self.item_tower(items)\n",
    "        users = nn.functional.pad(users, (0,60), value=1)\n",
    "        aggregation = users * items\n",
    "        return self.classifier(aggregation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchDataset(Dataset):\n",
    "    def __init__(self, idxs, data):\n",
    "        super().__init__()\n",
    "        self.idxs = idxs\n",
    "        self.data = data\n",
    "        self.size = idxs.shape[0]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        users_main_data = self.data['user']\n",
    "        items_main_data = self.data['item']\n",
    "        y_main_data = self.data['label']\n",
    "        \n",
    "        real_idxs = self.idxs[idx]\n",
    "        \n",
    "        users = users_main_data[real_idxs]\n",
    "        items = items_main_data[real_idxs]\n",
    "        y = y_main_data[real_idxs]\n",
    "        \n",
    "        return users, items, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory ./models/torch-models-2024-02-16T17-36-50\n",
      "Extracting Train set\n",
      "Extraction completed\n",
      "Converting to embeddings\n",
      "Convertion completed\n",
      "Training the model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0f0a0633fd34e74a17397b9ac72106a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs loop:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdebaf94f5ef4732ae46f8c1c4267994",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Stage train: iterating over batches:   0%|          | 0/180800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2608334280d64e01a0718ef6e310b55d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Stage validation: iterating over batches:   0%|          | 0/9516 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2cd0de7afcc4593b5baa51441890df3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Stage train: iterating over batches:   0%|          | 0/180800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "501b8b613b9141ee9cd31147405e29c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Stage validation: iterating over batches:   0%|          | 0/9516 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97de53afca6b452ba15930fae6a32b87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Stage train: iterating over batches:   0%|          | 0/180800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b90b3eccc4a04ae8a059e2bdabc52fb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Stage validation: iterating over batches:   0%|          | 0/9516 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78d9ef80ebbf426fac4958ef17c582ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Stage train: iterating over batches:   0%|          | 0/180800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee5effa4c68d4c9582db7a5da94b34eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Stage validation: iterating over batches:   0%|          | 0/9516 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2967705540c54439b716e9a1b8906e25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Stage train: iterating over batches:   0%|          | 0/180800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3afac852658c474e9597e7dccebec50c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Stage validation: iterating over batches:   0%|          | 0/9516 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6016ba12628492db3c2a2b179e25657",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Stage train: iterating over batches:   0%|          | 0/180800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c979e4adffbe4498aa9c943965f7ab39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Stage validation: iterating over batches:   0%|          | 0/9516 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "982042ae2fea4b0396032f5d1040a525",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Stage train: iterating over batches:   0%|          | 0/180800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4a57fdfd3df43fd8e72ed088ba5c5fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Stage validation: iterating over batches:   0%|          | 0/9516 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9133c6f13ca40bab2e54dc111ba7bfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Stage train: iterating over batches:   0%|          | 0/180800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ea54e076e294304959d019bc51e8b90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Stage validation: iterating over batches:   0%|          | 0/9516 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bace6b4ce1e4538b7f5d850184354d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Stage train: iterating over batches:   0%|          | 0/180800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20f6eded5e054d9c841818e263f5526d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Stage validation: iterating over batches:   0%|          | 0/9516 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44885952f7904c6eb9d5e21f991d2930",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Stage train: iterating over batches:   0%|          | 0/180800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7939555d3d66497f97682b304a30dac5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Stage validation: iterating over batches:   0%|          | 0/9516 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef1b6b9a9b664847855ea0942a80ce93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Stage train: iterating over batches:   0%|          | 0/180800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90b1609ac6db4bac8af963706fd7214d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Stage validation: iterating over batches:   0%|          | 0/9516 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfc72cdb7ee840a9b4c3c9ef916e461d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Stage train: iterating over batches:   0%|          | 0/180800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f499f8b3a682429b9f60ad32a57df959",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Stage validation: iterating over batches:   0%|          | 0/9516 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b465efadde6c46808d579a9401a3d989",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Stage train: iterating over batches:   0%|          | 0/180800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a3c5ef3613f4451961dbc3629e5e877",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Stage validation: iterating over batches:   0%|          | 0/9516 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4ab64ea2e444dc5bc5cbeb0ae9fa01c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Stage train: iterating over batches:   0%|          | 0/180800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7309ea244304dbc977ff54e90a5b458",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Stage validation: iterating over batches:   0%|          | 0/9516 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d18b10d60304fe78a511354622578e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Stage train: iterating over batches:   0%|          | 0/180800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "526122857ffe464583dde7e963e099d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Stage validation: iterating over batches:   0%|          | 0/9516 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35c3fc9deaa14c8da6ff20c153d60a31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Stage train: iterating over batches:   0%|          | 0/180800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab3643766d114907bcf98463760374d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Stage validation: iterating over batches:   0%|          | 0/9516 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cc21e03dc0e4457a85c5c0261e6a42e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Stage train: iterating over batches:   0%|          | 0/180800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "167cb11f844148abba1db99ce7431aa8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Stage validation: iterating over batches:   0%|          | 0/9516 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85b71f54f4a24f878bed6e31bc16eedc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Stage train: iterating over batches:   0%|          | 0/180800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f5e6bf37c73465587f7be1c36e0332f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Stage validation: iterating over batches:   0%|          | 0/9516 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff46ccaeac1b403a938cc864d66acb27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Stage train: iterating over batches:   0%|          | 0/180800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e122bd7fb4204e2dac3adc794d10a305",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Stage validation: iterating over batches:   0%|          | 0/9516 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76675b522dcb4b69a989ce54e47fd424",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Stage train: iterating over batches:   0%|          | 0/180800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class EarlyStopper:\n",
    "    def __init__(self, patience=1, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_validation_loss = float('inf')\n",
    "\n",
    "    def early_stop(self, validation_loss):\n",
    "        if validation_loss < self.min_validation_loss:\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "        elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "class Training:\n",
    "\n",
    "    def __init__(self, data_maker: TrainTestMaker, models_dir, data_size=None) -> None:\n",
    "        self.data_maker = data_maker\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        # self.device = torch.device(\"cpu\")\n",
    "        data = data_maker.get_train_set(data_size)\n",
    "        model = Model()\n",
    "        model = model.to(self.device)\n",
    "        # model = torch.compile(model)\n",
    "        model.eval()\n",
    "        self.model = model\n",
    "        self.validation_percent = 0.05\n",
    "        self.epochs = 20\n",
    "\n",
    "        classes, class_occurances = data['label'].unique(return_counts=True)\n",
    "        max_class_occurances = class_occurances.max()\n",
    "        weights = (max_class_occurances / class_occurances).to(self.device)\n",
    "        self._ce_weights = weights\n",
    "\n",
    "        self.criterion = torch.nn.CrossEntropyLoss(weight=weights, label_smoothing=0.15)\n",
    "        self.optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "        self._training_objects = dict()\n",
    "        self.data = data\n",
    "        self.models_dir = models_dir\n",
    "\n",
    "    def _save(self, models_dir, epoch=None):\n",
    "        if not os.path.isdir(models_dir):\n",
    "            os.mkdir(models_dir)\n",
    "        name = 'model-final.pt' if epoch is None else f'model-epoch-{epoch}.pt'\n",
    "        torch.save(self.model, os.path.join(models_dir, name))\n",
    "\n",
    "\n",
    "    def _build_loaders(self):\n",
    "        users = self.data['user']\n",
    "\n",
    "        permutation = np.random.permutation(len(users))\n",
    "        validation_size = int(len(users) * self.validation_percent)\n",
    "        validation_idxs = permutation[:validation_size]\n",
    "        train_idxs = permutation[validation_size:]\n",
    "        train_size = len(train_idxs)\n",
    "        \n",
    "        train_dataset = TorchDataset(train_idxs, self.data)\n",
    "        validation_dataset = TorchDataset(validation_idxs, self.data)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=128, num_workers=1, shuffle=True)\n",
    "        val_loader = DataLoader(validation_dataset, batch_size=128, num_workers=1, shuffle=False)\n",
    "        return train_loader, train_size, val_loader, validation_size\n",
    "\n",
    "    def _batch_train(self, data, stage):\n",
    "        users, items, labels = data\n",
    "        # mode to device/cuda\n",
    "        users, items, labels = users.to(self.device), items.to(self.device), labels.to(self.device)\n",
    "        # zero the parameter gradients\n",
    "        self.optimizer.zero_grad()\n",
    "        with torch.set_grad_enabled(stage == 'train'):\n",
    "            # logits: The raw predictions from the last layer            \n",
    "            logits = self.model((users, items))\n",
    "            _, preds = torch.max(logits, 1)\n",
    "            # return preds, labels\n",
    "            # using CE criterion, thus input is the logits\n",
    "            loss = self.criterion(logits, labels)\n",
    "            if stage == 'train':\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "        # statistics\n",
    "        running_loss = loss.item() * users.size(0)\n",
    "        running_corrects = torch.sum(preds == labels.argmax())\n",
    "        return running_loss, running_corrects\n",
    "\n",
    "    def _train(self, train_loader, train_size, val_loader, validation_size, models_dir):\n",
    "        metrics = {\n",
    "            'epochs': [],\n",
    "            'loss': {\n",
    "                'train': [],\n",
    "                'validation': [],\n",
    "            },\n",
    "            'accuracy': {\n",
    "                'train': [],\n",
    "                'validation': [],\n",
    "            }\n",
    "        }\n",
    "        print('Training the model')\n",
    "        early_stopper = EarlyStopper(patience=3, min_delta=0.1)\n",
    "        for epoch in tqdm(np.arange(self.epochs), desc=f'Epochs loop'):\n",
    "            for stage in ['train', 'validation']:\n",
    "                loss = self._epoch_train(stage, metrics, train_loader, train_size, val_loader, validation_size)\n",
    "                if stage == 'validation':\n",
    "                    validation_loss = loss\n",
    "            self._save(models_dir, epoch)\n",
    "            metrics['epochs'].append(epoch)\n",
    "            if early_stopper.early_stop(validation_loss):\n",
    "                print(f'Early stopping on epoch {epoch}')\n",
    "                break\n",
    "        print('Finished Training')\n",
    "        self._save(models_dir)\n",
    "        if not os.path.isdir(os.path.join('.', 'objects')):\n",
    "            os.mkdir(os.path.join('.', 'objects'))\n",
    "        with open(os.path.join(models_dir, 'training-metrics.pickle'), 'wb') as handle:\n",
    "            pickle.dump(metrics, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        return metrics\n",
    "\n",
    "\n",
    "    def _epoch_train(self, stage, metrics, train_loader, train_size, val_loader, validation_size):\n",
    "        epoch_loss = 0\n",
    "        epoch_accuracy = 0\n",
    "        if stage == 'train':\n",
    "            self.model.train()\n",
    "            loader = train_loader\n",
    "            dataset_size = train_size\n",
    "        else:\n",
    "            self.model.eval()\n",
    "            loader = val_loader\n",
    "            dataset_size = validation_size\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        # for i, data in enumerate(loader, 0):\n",
    "        for i, data in tqdm(enumerate(loader, 0), total=len(loader), desc=f'Stage {stage}: iterating over batches'):\n",
    "            # return _batch_train(data, stage)\n",
    "            current_loss, current_corrects = self._batch_train(data, stage)\n",
    "            running_loss += current_loss\n",
    "            running_corrects += current_corrects\n",
    "        epoch_loss = running_loss / dataset_size\n",
    "        epoch_accuracy = running_corrects.double() / dataset_size\n",
    "        # accuracy is still tensor\n",
    "        metrics['accuracy'][stage].append(epoch_accuracy.item())\n",
    "        # loss in python native type number\n",
    "        metrics['loss'][stage].append(epoch_loss)\n",
    "        return epoch_loss\n",
    "\n",
    "\n",
    "    def predict(self, dataset):\n",
    "        test_dataset = TorchDataset(torch.arange(dataset['user'].size(0)), dataset)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=64, num_workers=1, shuffle=False)\n",
    "        with torch.no_grad():\n",
    "            self.model.eval()\n",
    "            outputs = []\n",
    "            probabilities_acc = []\n",
    "            for users, items, labels in tqdm(test_loader, total=len(test_loader), desc=f'Prediction'):\n",
    "                users, items = users.to(self.device), items.to(self.device)\n",
    "                logits = self.model((users, items))\n",
    "                logits = self.model.softmax(logits)\n",
    "                probabilities = logits.cpu()\n",
    "                predictions = logits.argmax(dim=1).cpu()\n",
    "                outputs.append(predictions)\n",
    "                probabilities_acc.append(probabilities)\n",
    "            return torch.cat(outputs).numpy(), torch.cat(probabilities_acc).numpy()\n",
    "\n",
    "    def show_metrics(self, metrics, dirpath):\n",
    "        loss_df = pd.DataFrame(index=metrics['epochs'])\n",
    "        loss_df['train'] = metrics['loss']['train']\n",
    "        loss_df['validation'] = metrics['loss']['validation']\n",
    "        accuracy_df = pd.DataFrame(index=metrics['epochs'])\n",
    "        accuracy_df['train'] = metrics['accuracy']['train']\n",
    "        accuracy_df['validation'] = metrics['accuracy']['validation']\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 1, figsize=(10,10))\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        sns.lineplot(data=loss_df, ax=axes[0])\n",
    "        axes[0].grid()\n",
    "        axes[0].set_ylabel('Loss')\n",
    "        axes[0].set_xlabel('Epochs')\n",
    "\n",
    "        sns.lineplot(data=accuracy_df, ax=axes[1])\n",
    "        axes[1].grid()\n",
    "        axes[1].set_ylabel('Accuracy')\n",
    "        axes[1].set_xlabel('Epochs')\n",
    "        fig.tight_layout()\n",
    "        fig.savefig(os.path.join(dirpath, 'loss-accuracy-plots-during-training.png'))\n",
    "        plt.close(fig)\n",
    "\n",
    "    def run(self):\n",
    "        train_loader, train_size, val_loader, validation_size = self._build_loaders()\n",
    "        model = self.model.to(self.device)\n",
    "        res = self._train(train_loader, train_size, val_loader, validation_size, self.models_dir)\n",
    "        with open(os.path.join(self.models_dir, 'training-metrics.pickle'), 'rb') as handle:\n",
    "            metrics = pickle.load(handle)\n",
    "        self.show_metrics(metrics, self.models_dir)\n",
    "        del self.data\n",
    "        # collect garbage\n",
    "        gc.collect()\n",
    "        test_set_hot_users, test_set_cold_users = data_maker.get_test_set()\n",
    "        predictions, probabilities = self.predict(test_set_hot_users)\n",
    "        report = pd.DataFrame()\n",
    "        \n",
    "        report['ground-truth'] = test_set_hot_users['label'].numpy().astype('int').argmax(axis=1)\n",
    "        report['predictions'] = predictions\n",
    "        report['prediction-proba-0'] = probabilities[:, 0]\n",
    "        report['prediction-proba-1'] = probabilities[:, 1]\n",
    "        report.to_csv(os.path.join(self.models_dir, 'hot-users-predictions.csv'), index=False)\n",
    "\n",
    "        del test_set_hot_users\n",
    "\n",
    "        predictions, probabilities = self.predict(test_set_cold_users)\n",
    "        report = pd.DataFrame()\n",
    "        report['ground-truth'] = test_set_cold_users['label'].numpy().astype('int').argmax(axis=1)\n",
    "        report['predictions'] = predictions\n",
    "        report['prediction-proba-0'] = probabilities[:, 0]\n",
    "        report['prediction-proba-1'] = probabilities[:, 1]\n",
    "        report.to_csv(os.path.join(self.models_dir, 'cold-users-predictions.csv'), index=False)\n",
    "        del test_set_cold_users\n",
    "\n",
    "\n",
    "def run():\n",
    "    current_datetime = datetime.datetime.now()\n",
    "    date_str = current_datetime.strftime(\"%Y-%m-%d\")\n",
    "    time_str = current_datetime.strftime(\"%H-%M-%S\")\n",
    "    models_dir = os.path.join('.', 'models', f\"torch-models-{date_str}T{time_str}\")\n",
    "    print(f'Directory {models_dir}')\n",
    "    if not os.path.isdir(models_dir):\n",
    "        os.mkdir(models_dir)\n",
    "    train_manager = Training(\n",
    "        data_maker,\n",
    "        models_dir=models_dir,\n",
    "        # data_size=3_000_000,\n",
    "    )\n",
    "    train_manager.run()\n",
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
