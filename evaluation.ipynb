{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "from sklearn.preprocessing import LabelEncoder \n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import datetime\n",
    "import pickle\n",
    "from pprint import pprint\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import seaborn as sns\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CategoricalFeature:\n",
    "    def __init__(self, name, vocabulary_size, embedding_size) -> None:\n",
    "        # + 1 for unknown\n",
    "        self.lookup_table = nn.Embedding(vocabulary_size + 1, embedding_size)\n",
    "        self.name = name\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "\n",
    "    def to_embeddings(self, idxs):\n",
    "        if not torch.is_tensor(idxs):\n",
    "            idxs = torch.tensor(idxs)\n",
    "        with torch.no_grad():\n",
    "            embeddings = self.lookup_table(idxs)\n",
    "        return embeddings\n",
    "\n",
    "class FeaturesEncoder:\n",
    "    @staticmethod\n",
    "    def get_instance(name: str, features_numeric, features_categoric = None):\n",
    "        if features_categoric is None:\n",
    "            filepath = os.path.join('.', 'lookup_tables', f'{name}-{\"-\".join(features_numeric)}.pickle')\n",
    "        else:\n",
    "            filepath = os.path.join('.', 'lookup_tables', f'{name}-{\"-\".join(features_numeric)}-{\"-\".join(features_categoric)}.pickle')\n",
    "        if os.path.exists(filepath):\n",
    "            with open(filepath, 'rb') as handle:\n",
    "                encoder = pickle.load(handle)\n",
    "            print(f'{name.capitalize()} loaded from {filepath}')\n",
    "        else:\n",
    "            encoder = FeaturesEncoder(features_numeric, features_categoric)\n",
    "            with open(filepath, 'wb') as handle:\n",
    "                pickle.dump(encoder, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            print(f'{name.capitalize()} initialized and saved to {filepath}')\n",
    "        return encoder\n",
    "    \n",
    "    def __init__(self, features_numeric, features_categoric ) -> None:\n",
    "        self.features_numeric = features_numeric\n",
    "        self.features_categoric = [] if features_categoric is None else list(features_categoric.keys())\n",
    "        self.features_categoric_lookup_table = dict() if features_categoric is None else dict([\n",
    "            (feature_name, CategoricalFeature(\n",
    "                name=feature_name,\n",
    "                vocabulary_size=vocabulary_size,\n",
    "                embedding_size=embedding_size,\n",
    "            ))\n",
    "            for feature_name, (vocabulary_size, embedding_size) in features_categoric.items()\n",
    "        ])\n",
    "\n",
    "    def to_emebeddings(self, df: pd.DataFrame):\n",
    "        numeric_features = df[self.features_numeric].to_numpy()\n",
    "        df.drop(columns=self.features_numeric, inplace=True)\n",
    "        numeric_features = torch.from_numpy(numeric_features).float()\n",
    "        categoric_features = []\n",
    "        for feature in self.features_categoric:\n",
    "            encoder = self.features_categoric_lookup_table[feature]\n",
    "            categoric_feature = df[feature].to_numpy()\n",
    "            df.drop(columns=[feature], inplace=True)\n",
    "            categoric_feature = torch.from_numpy(categoric_feature)\n",
    "            feature_embedding = encoder.to_embeddings(categoric_feature)\n",
    "            categoric_features.append(feature_embedding)\n",
    "        # concat (numeric, ...categoric)\n",
    "        return torch.cat((numeric_features, *categoric_features), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainTestMaker:\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        data = TrainTestMaker.read_all_data()\n",
    "        self.data: pd.DataFrame = data\n",
    "        user_features_numeric, item_features_numeric, item_features_categoric = TrainTestMaker.get_features_defs(data)\n",
    "        self.user_encoder = FeaturesEncoder.get_instance('user', user_features_numeric)\n",
    "        self.item_encoder = FeaturesEncoder.get_instance('item', item_features_numeric, item_features_categoric)\n",
    "\n",
    "    @staticmethod\n",
    "    def read_all_data() -> pd.DataFrame:\n",
    "        paths = []\n",
    "        dirs = os.listdir('normalized_train_data')\n",
    "        for dir in dirs:\n",
    "            filenames = os.listdir(f'normalized_train_data/{dir}')\n",
    "            paths += [f'normalized_train_data/{dir}/{filename}' for filename in filenames]\n",
    "        paths = sorted(paths)\n",
    "        dfs = []\n",
    "        for filepath in tqdm(paths, desc='Loading data'):\n",
    "            df = pd.read_csv(filepath)\n",
    "            df.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "            dfs.append(df)\n",
    "        dfs = pd.concat(dfs)\n",
    "        dfs.reset_index(drop=True, inplace=True)\n",
    "        return dfs\n",
    "\n",
    "    @staticmethod\n",
    "    def load_encoders():\n",
    "        features = [\n",
    "            'user_id_hash',\n",
    "            'target_id_hash',\n",
    "            'syndicator_id_hash',\n",
    "            'campaign_id_hash',\n",
    "            'target_item_taxonomy',\n",
    "            'placement_id_hash',\n",
    "            'publisher_id_hash',\n",
    "            'source_id_hash',\n",
    "            'source_item_type',\n",
    "            'browser_platform',\n",
    "            'country_code',\n",
    "            'region',\n",
    "        ]\n",
    "        column_encoders = dict()\n",
    "        for feature in tqdm(features, desc='Loading encoders'):\n",
    "            with open(f'./label_encoders/{feature}.pickle', 'rb') as handle:\n",
    "                encoder = pickle.load(handle)\n",
    "            column_encoders[feature] = encoder\n",
    "        return column_encoders\n",
    "\n",
    "    @staticmethod\n",
    "    def get_features_defs(data):\n",
    "        user_features_numeric = [\n",
    "            'page_view_start_time',\n",
    "            'user_recs',\n",
    "            'user_clicks', \n",
    "            'user_target_recs',\n",
    "        ]\n",
    "\n",
    "        item_features_numeric = [\n",
    "            'page_view_start_time',\n",
    "            'empiric_calibrated_recs',\n",
    "            'empiric_clicks',\n",
    "        ]\n",
    "\n",
    "        item_features_categoric = {\n",
    "            # (vocabulary size, embedding size)\n",
    "            'syndicator_id_hash': (data['syndicator_id_hash'].nunique(), 32),\n",
    "            'campaign_id_hash': (data['campaign_id_hash'].nunique(), 32),\n",
    "            'placement_id_hash': (data['placement_id_hash'].nunique(), 32),\n",
    "            'target_item_taxonomy': (data['target_item_taxonomy'].nunique(), 8),\n",
    "        }\n",
    "        return user_features_numeric, item_features_numeric, item_features_categoric\n",
    "\n",
    "\n",
    "    def _get_split(self):\n",
    "        max_timestamp = self.data['page_view_start_time'].max()\n",
    "        three_days_ago = (datetime.datetime.fromtimestamp(max_timestamp/1000) - datetime.timedelta(days=3)).replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "        three_days_ago = int(datetime.datetime.timestamp(three_days_ago) * 1000)\n",
    "        return three_days_ago\n",
    "\n",
    "    def get_trainset_df(self):\n",
    "        threshold = self._get_split()\n",
    "        train_set = self.data[self.data['page_view_start_time'] < threshold]\n",
    "        train_set.reset_index(drop=True, inplace=True)\n",
    "        return train_set\n",
    "\n",
    "    def df_to_embeddings(self, df):\n",
    "        batch_size = 1_000_000\n",
    "        user_embeddings = []\n",
    "        item_embeddings = []\n",
    "        for i in range(0, len(df.index), batch_size):\n",
    "            batch = df.iloc[i:i + batch_size]\n",
    "            user_features_df = batch[self.user_encoder.features_numeric + self.user_encoder.features_categoric].copy(deep=True)\n",
    "            del batch\n",
    "            batch_user_embeddings = self.user_encoder.to_emebeddings(user_features_df)\n",
    "            user_embeddings.append(batch_user_embeddings)\n",
    "            del user_features_df\n",
    "            batch = df.iloc[i:i + batch_size]\n",
    "            item_features_df = batch[self.item_encoder.features_numeric + self.item_encoder.features_categoric].copy(deep=True)\n",
    "            del batch\n",
    "            batch_item_embeddings = self.item_encoder.to_emebeddings(item_features_df)\n",
    "            item_embeddings.append(batch_item_embeddings)\n",
    "            del item_features_df\n",
    "        user_embeddings_concatenated = torch.cat(user_embeddings)\n",
    "        del user_embeddings\n",
    "        item_embeddings_concatenated = torch.cat(item_embeddings)\n",
    "        del item_embeddings\n",
    "        data = {\n",
    "            'user': user_embeddings_concatenated,\n",
    "            'item': item_embeddings_concatenated,\n",
    "        }\n",
    "        if 'is_click' in df.columns:\n",
    "            labels = df['is_click'].to_numpy().astype('int')\n",
    "            labels_one_hot = np.zeros((labels.shape[0], 2))\n",
    "            labels_one_hot[np.arange(labels.shape[0]), labels] = 1\n",
    "            del labels\n",
    "            data['label'] = torch.from_numpy(labels_one_hot).float()\n",
    "        return data\n",
    "    \n",
    "    def get_test_set_df(self):\n",
    "        threshold = self._get_split()\n",
    "        train_set = self.data[self.data['page_view_start_time'] < threshold]\n",
    "        last_three_days = self.data[self.data['page_view_start_time'] >= threshold]\n",
    "        not_cold_users_mask = last_three_days['user_id_hash'].isin(train_set['user_id_hash'])\n",
    "        test_set_hot_users = last_three_days[not_cold_users_mask]\n",
    "        test_set_cold_users = last_three_days[~not_cold_users_mask]\n",
    "        test_set_hot_users.reset_index(drop=True, inplace=True)\n",
    "        test_set_cold_users.reset_index(drop=True, inplace=True)\n",
    "        return test_set_hot_users, test_set_cold_users\n",
    "\n",
    "    def get_train_set(self, max_idx = None):\n",
    "        print('Extracting Train set')\n",
    "        train_set_df = self.get_trainset_df()\n",
    "        print('Extraction completed')\n",
    "        if max_idx is not None:\n",
    "            train_set_df = train_set_df.iloc[: max_idx].copy()\n",
    "        print('Converting to embeddings')\n",
    "        train_set = self.df_to_embeddings(train_set_df)\n",
    "        print('Convertion completed')\n",
    "        return train_set\n",
    "    \n",
    "    def get_test_set(self):\n",
    "        test_set_hot_users_df, test_set_cold_users_df = self.get_test_set_df()\n",
    "        test_set_hot_users = self.df_to_embeddings(test_set_hot_users_df)\n",
    "        del test_set_hot_users_df\n",
    "        test_set_cold_users = self.df_to_embeddings(test_set_cold_users_df)\n",
    "        del test_set_cold_users_df\n",
    "        return test_set_hot_users, test_set_cold_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd0290b1f0284dfa8f595abdd9cfb5cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading data:   0%|          | 0/69 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User loaded from .\\lookup_tables\\user-page_view_start_time-user_recs-user_clicks-user_target_recs.pickle\n",
      "Item loaded from .\\lookup_tables\\item-page_view_start_time-empiric_calibrated_recs-empiric_clicks-syndicator_id_hash-campaign_id_hash-placement_id_hash-target_item_taxonomy.pickle\n"
     ]
    }
   ],
   "source": [
    "data_maker = TrainTestMaker()\n",
    "# train_set = data_maker.get_train_set(3_000_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        self.user_tower = nn.Sequential(\n",
    "            nn.Linear(4, 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4, 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4, 4),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.item_tower = nn.Sequential(\n",
    "            nn.Linear(107, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        users, items = x\n",
    "        users = self.user_tower(users)\n",
    "        items = self.item_tower(items)\n",
    "        users = nn.functional.pad(users, (0,60), value=1)\n",
    "        aggregation = users * items\n",
    "        return self.classifier(aggregation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_test_set():\n",
    "    encoders = data_maker.load_encoders()\n",
    "    test_df = pd.read_csv('./test_file.csv')\n",
    "    test_df['country_code'].fillna('Null', inplace=True)\n",
    "    test_df['region'].fillna('Null', inplace=True)\n",
    "    test_df.drop(columns=['user_id_hash', 'target_id_hash'], inplace=True)\n",
    "    for column, encoder in tqdm(encoders.items(), desc='Normalizing test file'):\n",
    "        if column in test_df.columns:\n",
    "            print(f'Encoding {column}')\n",
    "            unique_values = test_df[column].unique()\n",
    "            value_to_code = dict()\n",
    "            for value in unique_values:\n",
    "                try:\n",
    "                    mapped_value = encoder.transform([value])[0]\n",
    "                except ValueError as err:\n",
    "                    mapped_value = encoder.classes_.shape[0]\n",
    "                \n",
    "                value_to_code[value] = mapped_value\n",
    "            test_df[column] = test_df[column].apply(lambda value: value_to_code[value])\n",
    "    return test_df\n",
    "# test_df = normalize_test_set()\n",
    "# test_df.to_csv('./test_file_normalized.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7cb1a47a7c14a27b257811c54d6edee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading encoders:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "encoders = data_maker.load_encoders()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_encoder = encoders['user_id_hash']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_ids_df = pd.read_csv('./normalized_data/user_id_hash.csv')\n",
    "user_ids_df.reset_index(inplace=True)\n",
    "user_ids_df.set_index('user_id_hash', inplace=True)\n",
    "user_ids_df = user_ids_df['index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_ids_dict = user_ids_df.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16264937"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(user_ids_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16264937"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_ids_df.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_raw = pd.read_csv('./test_file.csv', usecols=['user_id_hash'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_user_ids_normalized = test_df_raw['user_id_hash'].map(user_ids_dict.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         None\n",
       "1         None\n",
       "2         None\n",
       "3         None\n",
       "4         None\n",
       "          ... \n",
       "444501    None\n",
       "444502    None\n",
       "444503    None\n",
       "444504    None\n",
       "444505    None\n",
       "Name: user_id_hash, Length: 444506, dtype: object"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_user_ids_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(444506, 0)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_user_ids_normalized.index), test_user_ids_normalized.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def temp():\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding user_id_hash\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd8f199e10144ae291ab49c177c7c774",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Transfroming unique values:   0%|          | 0/422613 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 24\u001b[0m\n\u001b[0;32m     22\u001b[0m             test_df[column] \u001b[38;5;241m=\u001b[39m test_df[column]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m value: value_to_code[value])\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m test_df\n\u001b[1;32m---> 24\u001b[0m test_df \u001b[38;5;241m=\u001b[39m \u001b[43mnormalize_test_set_all_columns\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# test_df.to_csv('./test_file_normalized_all_columns.csv')\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[16], line 16\u001b[0m, in \u001b[0;36mnormalize_test_set_all_columns\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m value \u001b[38;5;129;01min\u001b[39;00m tqdm(unique_values, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTransfroming unique values\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 16\u001b[0m         mapped_value \u001b[38;5;241m=\u001b[39m \u001b[43mencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m     18\u001b[0m         mapped_value \u001b[38;5;241m=\u001b[39m encoder\u001b[38;5;241m.\u001b[39mclasses_\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Mark\\anaconda3\\envs\\research\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 140\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    142\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    143\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    145\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    146\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\Mark\\anaconda3\\envs\\research\\Lib\\site-packages\\sklearn\\preprocessing\\_label.py:139\u001b[0m, in \u001b[0;36mLabelEncoder.transform\u001b[1;34m(self, y)\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _num_samples(y) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([])\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_encode\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muniques\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclasses_\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Mark\\anaconda3\\envs\\research\\Lib\\site-packages\\sklearn\\utils\\_encode.py:224\u001b[0m, in \u001b[0;36m_encode\u001b[1;34m(values, uniques, check_unknown)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m values\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOUS\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    223\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 224\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_map_to_integer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muniques\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    225\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    226\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my contains previously unseen labels: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Mark\\anaconda3\\envs\\research\\Lib\\site-packages\\sklearn\\utils\\_encode.py:163\u001b[0m, in \u001b[0;36m_map_to_integer\u001b[1;34m(values, uniques)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_map_to_integer\u001b[39m(values, uniques):\n\u001b[0;32m    162\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Map values based on its position in uniques.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 163\u001b[0m     table \u001b[38;5;241m=\u001b[39m \u001b[43m_nandict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[43mval\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43muniques\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([table[v] \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m values])\n",
      "File \u001b[1;32mc:\\Users\\Mark\\anaconda3\\envs\\research\\Lib\\site-packages\\sklearn\\utils\\_encode.py:151\u001b[0m, in \u001b[0;36m_nandict.__init__\u001b[1;34m(self, mapping)\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(mapping)\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m mapping\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m--> 151\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_scalar_nan(key):\n\u001b[0;32m    152\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnan_value \u001b[38;5;241m=\u001b[39m value\n\u001b[0;32m    153\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def normalize_test_set_all_columns():\n",
    "    # encoders = data_maker.load_encoders()\n",
    "    test_df_raw = pd.read_csv('./test_file.csv')\n",
    "    test_df = pd.read_csv('./test_file_normalized.csv')\n",
    "    test_df.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "    test_df['user_id_hash'] = test_df_raw['user_id_hash']\n",
    "    test_df['target_id_hash'] = test_df_raw['target_id_hash']\n",
    "    for column in ['user_id_hash', 'target_id_hash']:\n",
    "        encoder = encoders[column]\n",
    "        if column in test_df.columns:\n",
    "            print(f'Encoding {column}')\n",
    "            unique_values = test_df[column].unique()\n",
    "            value_to_code = dict()\n",
    "            for value in tqdm(unique_values, desc='Transfroming unique values'):\n",
    "                try:\n",
    "                    mapped_value = encoder.transform([value])[0]\n",
    "                except ValueError as err:\n",
    "                    mapped_value = encoder.classes_.shape[0]\n",
    "                \n",
    "                value_to_code[value] = mapped_value\n",
    "            print('Applying mapping')\n",
    "            test_df[column] = test_df[column].apply(lambda value: value_to_code[value])\n",
    "    return test_df\n",
    "test_df = normalize_test_set_all_columns()\n",
    "# test_df.to_csv('./test_file_normalized_all_columns.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading normalized testset\n",
      "Generating embeddings\n",
      "Embeddings generated\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c542078418524535a794f3df63de23bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Prediction:   0%|          | 0/6946 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class TorchDataset(Dataset):\n",
    "    def __init__(self, idxs, data):\n",
    "        super().__init__()\n",
    "        self.idxs = idxs\n",
    "        self.data = data\n",
    "        self.size = idxs.shape[0]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        users_main_data = self.data['user']\n",
    "        items_main_data = self.data['item']\n",
    "        \n",
    "        real_idxs = self.idxs[idx]\n",
    "        \n",
    "        users = users_main_data[real_idxs]\n",
    "        items = items_main_data[real_idxs]\n",
    "        \n",
    "        return users, items\n",
    "\n",
    "class TestsetEval:\n",
    "\n",
    "    def __init__(self, model_filepath) -> None:\n",
    "        # model = torch.load(model_filepath)\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model = torch.load(model_filepath, map_location=self.device)\n",
    "        model = model.to(self.device)\n",
    "        model.eval()\n",
    "        self.model = model\n",
    "\n",
    "    @staticmethod\n",
    "    def load_normalized_test_set():\n",
    "        test_df = pd.read_csv('./test_file_normalized.csv')\n",
    "        test_df.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "        return test_df\n",
    "    \n",
    "\n",
    "    def predict(self):\n",
    "        print('Loading normalized testset')\n",
    "        test_df = TestsetEval.load_normalized_test_set()\n",
    "        print('Generating embeddings')\n",
    "        dataset = data_maker.df_to_embeddings(test_df)\n",
    "        print('Embeddings generated')\n",
    "        test_dataset = TorchDataset(torch.arange(dataset['user'].size(0)), dataset)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=64, num_workers=1, shuffle=False)\n",
    "        with torch.no_grad():\n",
    "            self.model.eval()\n",
    "            outputs = []\n",
    "            probabilities_acc = []\n",
    "            for users, items in tqdm(test_loader, total=len(test_loader), desc=f'Prediction'):\n",
    "                users, items = users.to(self.device), items.to(self.device)\n",
    "                logits = self.model((users, items))\n",
    "                logits = self.model.softmax(logits)\n",
    "                probabilities = logits.cpu()\n",
    "                predictions = logits.argmax(dim=1).cpu()\n",
    "                outputs.append(predictions)\n",
    "                probabilities_acc.append(probabilities)\n",
    "            return torch.cat(outputs).numpy(), torch.cat(probabilities_acc).numpy()\n",
    "        \n",
    "testset_eval = TestsetEval('./models/torch-models-2024-02-16T17-36-50/model-final.pt')\n",
    "predictions, probabilities = testset_eval.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def report_submission():\n",
    "    pred = probabilities[:,1]\n",
    "    pred_df = pd.DataFrame(pred)\n",
    "    pred_df.reset_index(inplace=True)\n",
    "    pred_df.columns = ['Id','Predicted']\n",
    "    pred_df.to_csv('my_submission.csv',index=False)\n",
    "    return pd.read_csv('./my_submission.csv').head(15)\n",
    "report_submission()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
