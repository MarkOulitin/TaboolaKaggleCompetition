{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "from sklearn.preprocessing import LabelEncoder \n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import datetime\n",
    "import pickle\n",
    "from pprint import pprint\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import seaborn as sns\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CategoricalFeature:\n",
    "    def __init__(self, name, vocabulary_size, embedding_size) -> None:\n",
    "        # + 1 for unknown\n",
    "        self.lookup_table = nn.Embedding(vocabulary_size + 1, embedding_size)\n",
    "        self.name = name\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "\n",
    "    def to_embeddings(self, idxs):\n",
    "        if not torch.is_tensor(idxs):\n",
    "            idxs = torch.tensor(idxs)\n",
    "        with torch.no_grad():\n",
    "            embeddings = self.lookup_table(idxs)\n",
    "        return embeddings\n",
    "\n",
    "class FeaturesEncoder:\n",
    "    @staticmethod\n",
    "    def get_instance(name: str, features_numeric, features_categoric = None):\n",
    "        if features_categoric is None:\n",
    "            filepath = os.path.join('.', 'lookup_tables', f'{name}-{\"-\".join(features_numeric)}.pickle')\n",
    "        else:\n",
    "            filepath = os.path.join('.', 'lookup_tables', f'{name}-{\"-\".join(features_numeric)}-{\"-\".join(features_categoric)}.pickle')\n",
    "        if os.path.exists(filepath):\n",
    "            with open(filepath, 'rb') as handle:\n",
    "                encoder = pickle.load(handle)\n",
    "            print(f'{name.capitalize()} loaded from {filepath}')\n",
    "        else:\n",
    "            encoder = FeaturesEncoder(features_numeric, features_categoric)\n",
    "            with open(filepath, 'wb') as handle:\n",
    "                pickle.dump(encoder, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            print(f'{name.capitalize()} initialized and saved to {filepath}')\n",
    "        return encoder\n",
    "    \n",
    "    def __init__(self, features_numeric, features_categoric ) -> None:\n",
    "        self.features_numeric = features_numeric\n",
    "        self.features_categoric = [] if features_categoric is None else list(features_categoric.keys())\n",
    "        self.features_categoric_lookup_table = dict() if features_categoric is None else dict([\n",
    "            (feature_name, CategoricalFeature(\n",
    "                name=feature_name,\n",
    "                vocabulary_size=vocabulary_size,\n",
    "                embedding_size=embedding_size,\n",
    "            ))\n",
    "            for feature_name, (vocabulary_size, embedding_size) in features_categoric.items()\n",
    "        ])\n",
    "\n",
    "    def to_emebeddings(self, df: pd.DataFrame):\n",
    "        numeric_features = df[self.features_numeric].to_numpy()\n",
    "        df.drop(columns=self.features_numeric, inplace=True)\n",
    "        numeric_features = torch.from_numpy(numeric_features).float()\n",
    "        categoric_features = []\n",
    "        for feature in self.features_categoric:\n",
    "            encoder = self.features_categoric_lookup_table[feature]\n",
    "            categoric_feature = df[feature].to_numpy()\n",
    "            df.drop(columns=[feature], inplace=True)\n",
    "            categoric_feature = torch.from_numpy(categoric_feature)\n",
    "            feature_embedding = encoder.to_embeddings(categoric_feature)\n",
    "            categoric_features.append(feature_embedding)\n",
    "        # concat (numeric, ...categoric)\n",
    "        return torch.cat((numeric_features, *categoric_features), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainTestMaker:\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        data = TrainTestMaker.read_all_data()\n",
    "        self.data: pd.DataFrame = data\n",
    "        user_features_numeric, item_features_numeric, item_features_categoric = TrainTestMaker.get_features_defs(data)\n",
    "        self.user_encoder = FeaturesEncoder.get_instance('user', user_features_numeric)\n",
    "        self.item_encoder = FeaturesEncoder.get_instance('item', item_features_numeric, item_features_categoric)\n",
    "\n",
    "    @staticmethod\n",
    "    def read_all_data() -> pd.DataFrame:\n",
    "        paths = []\n",
    "        dirs = os.listdir('normalized_train_data')\n",
    "        for dir in dirs:\n",
    "            filenames = os.listdir(f'normalized_train_data/{dir}')\n",
    "            paths += [f'normalized_train_data/{dir}/{filename}' for filename in filenames]\n",
    "        paths = sorted(paths)\n",
    "        dfs = []\n",
    "        for filepath in tqdm(paths, desc='Loading data'):\n",
    "            df = pd.read_csv(filepath)\n",
    "            df.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "            dfs.append(df)\n",
    "        dfs = pd.concat(dfs)\n",
    "        dfs.reset_index(drop=True, inplace=True)\n",
    "        return dfs\n",
    "\n",
    "    @staticmethod\n",
    "    def load_encoders():\n",
    "        features = [\n",
    "            'user_id_hash',\n",
    "            'target_id_hash',\n",
    "            'syndicator_id_hash',\n",
    "            'campaign_id_hash',\n",
    "            'target_item_taxonomy',\n",
    "            'placement_id_hash',\n",
    "            'publisher_id_hash',\n",
    "            'source_id_hash',\n",
    "            'source_item_type',\n",
    "            'browser_platform',\n",
    "            'country_code',\n",
    "            'region',\n",
    "        ]\n",
    "        column_encoders = dict()\n",
    "        for feature in tqdm(features, desc='Loading encoders'):\n",
    "            with open(f'./label_encoders/{feature}.pickle', 'rb') as handle:\n",
    "                encoder = pickle.load(handle)\n",
    "            column_encoders[feature] = encoder\n",
    "        return column_encoders\n",
    "\n",
    "    @staticmethod\n",
    "    def get_features_defs(data):\n",
    "        user_features_numeric = [\n",
    "            'page_view_start_time',\n",
    "            'user_recs',\n",
    "            'user_clicks', \n",
    "            'user_target_recs',\n",
    "        ]\n",
    "\n",
    "        item_features_numeric = [\n",
    "            'page_view_start_time',\n",
    "            'empiric_calibrated_recs',\n",
    "            'empiric_clicks',\n",
    "        ]\n",
    "\n",
    "        item_features_categoric = {\n",
    "            # (vocabulary size, embedding size)\n",
    "            'syndicator_id_hash': (data['syndicator_id_hash'].nunique(), 32),\n",
    "            'campaign_id_hash': (data['campaign_id_hash'].nunique(), 32),\n",
    "            'placement_id_hash': (data['placement_id_hash'].nunique(), 32),\n",
    "            'target_item_taxonomy': (data['target_item_taxonomy'].nunique(), 8),\n",
    "        }\n",
    "        return user_features_numeric, item_features_numeric, item_features_categoric\n",
    "\n",
    "\n",
    "    def _get_split(self):\n",
    "        max_timestamp = self.data['page_view_start_time'].max()\n",
    "        three_days_ago = (datetime.datetime.fromtimestamp(max_timestamp/1000) - datetime.timedelta(days=3)).replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "        three_days_ago = int(datetime.datetime.timestamp(three_days_ago) * 1000)\n",
    "        return three_days_ago\n",
    "\n",
    "    def get_trainset_df(self):\n",
    "        threshold = self._get_split()\n",
    "        train_set = self.data[self.data['page_view_start_time'] < threshold]\n",
    "        train_set.reset_index(drop=True, inplace=True)\n",
    "        return train_set\n",
    "\n",
    "    def df_to_embeddings(self, df):\n",
    "        batch_size = 1_000_000\n",
    "        user_embeddings = []\n",
    "        item_embeddings = []\n",
    "        for i in range(0, len(df.index), batch_size):\n",
    "            batch = df.iloc[i:i + batch_size]\n",
    "            user_features_df = batch[self.user_encoder.features_numeric + self.user_encoder.features_categoric].copy(deep=True)\n",
    "            del batch\n",
    "            batch_user_embeddings = self.user_encoder.to_emebeddings(user_features_df)\n",
    "            user_embeddings.append(batch_user_embeddings)\n",
    "            del user_features_df\n",
    "            batch = df.iloc[i:i + batch_size]\n",
    "            item_features_df = batch[self.item_encoder.features_numeric + self.item_encoder.features_categoric].copy(deep=True)\n",
    "            del batch\n",
    "            batch_item_embeddings = self.item_encoder.to_emebeddings(item_features_df)\n",
    "            item_embeddings.append(batch_item_embeddings)\n",
    "            del item_features_df\n",
    "        user_embeddings_concatenated = torch.cat(user_embeddings)\n",
    "        del user_embeddings\n",
    "        item_embeddings_concatenated = torch.cat(item_embeddings)\n",
    "        del item_embeddings\n",
    "        data = {\n",
    "            'user': user_embeddings_concatenated,\n",
    "            'item': item_embeddings_concatenated,\n",
    "        }\n",
    "        if 'is_click' in df.columns:\n",
    "            labels = df['is_click'].to_numpy().astype('int')\n",
    "            labels_one_hot = np.zeros((labels.shape[0], 2))\n",
    "            labels_one_hot[np.arange(labels.shape[0]), labels] = 1\n",
    "            del labels\n",
    "            data['label'] = torch.from_numpy(labels_one_hot).float()\n",
    "        return data\n",
    "    \n",
    "    def get_test_set_df(self):\n",
    "        threshold = self._get_split()\n",
    "        train_set = self.data[self.data['page_view_start_time'] < threshold]\n",
    "        last_three_days = self.data[self.data['page_view_start_time'] >= threshold]\n",
    "        not_cold_users_mask = last_three_days['user_id_hash'].isin(train_set['user_id_hash'])\n",
    "        test_set_hot_users = last_three_days[not_cold_users_mask]\n",
    "        test_set_cold_users = last_three_days[~not_cold_users_mask]\n",
    "        test_set_hot_users.reset_index(drop=True, inplace=True)\n",
    "        test_set_cold_users.reset_index(drop=True, inplace=True)\n",
    "        return test_set_hot_users, test_set_cold_users\n",
    "\n",
    "    def get_train_set(self, max_idx = None):\n",
    "        print('Extracting Train set')\n",
    "        train_set_df = self.get_trainset_df()\n",
    "        print('Extraction completed')\n",
    "        if max_idx is not None:\n",
    "            train_set_df = train_set_df.iloc[: max_idx].copy()\n",
    "        print('Converting to embeddings')\n",
    "        train_set = self.df_to_embeddings(train_set_df)\n",
    "        print('Convertion completed')\n",
    "        return train_set\n",
    "    \n",
    "    def get_test_set(self):\n",
    "        test_set_hot_users_df, test_set_cold_users_df = self.get_test_set_df()\n",
    "        test_set_hot_users = self.df_to_embeddings(test_set_hot_users_df)\n",
    "        del test_set_hot_users_df\n",
    "        test_set_cold_users = self.df_to_embeddings(test_set_cold_users_df)\n",
    "        del test_set_cold_users_df\n",
    "        return test_set_hot_users, test_set_cold_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82cdd775d9e645f6bb578b4e81c531e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading data:   0%|          | 0/69 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User loaded from ./lookup_tables/user-page_view_start_time-user_recs-user_clicks-user_target_recs.pickle\n",
      "Item loaded from ./lookup_tables/item-page_view_start_time-empiric_calibrated_recs-empiric_clicks-syndicator_id_hash-campaign_id_hash-placement_id_hash-target_item_taxonomy.pickle\n"
     ]
    }
   ],
   "source": [
    "data_maker = TrainTestMaker()\n",
    "# train_set = data_maker.get_train_set(3_000_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        self.user_tower = nn.Sequential(\n",
    "            nn.Linear(4, 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4, 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4, 4),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.item_tower = nn.Sequential(\n",
    "            nn.Linear(107, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        users, items = x\n",
    "        users = self.user_tower(users)\n",
    "        items = self.item_tower(items)\n",
    "        users = nn.functional.pad(users, (0,60), value=1)\n",
    "        aggregation = users * items\n",
    "        return self.classifier(aggregation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_test_set():\n",
    "    encoders = data_maker.load_encoders()\n",
    "    test_df = pd.read_csv('./test_file.csv')\n",
    "    test_df['country_code'].fillna('Null', inplace=True)\n",
    "    test_df['region'].fillna('Null', inplace=True)\n",
    "    test_df.drop(columns=['user_id_hash', 'target_id_hash'], inplace=True)\n",
    "    for column, encoder in tqdm(encoders.items(), desc='Normalizing test file'):\n",
    "        if column in test_df.columns:\n",
    "            print(f'Encoding {column}')\n",
    "            unique_values = test_df[column].unique()\n",
    "            value_to_code = dict()\n",
    "            for value in unique_values:\n",
    "                try:\n",
    "                    mapped_value = encoder.transform([value])[0]\n",
    "                except ValueError as err:\n",
    "                    mapped_value = encoder.classes_.shape[0]\n",
    "                \n",
    "                value_to_code[value] = mapped_value\n",
    "            test_df[column] = test_df[column].apply(lambda value: value_to_code[value])\n",
    "    return test_df\n",
    "# test_df = normalize_test_set()\n",
    "# test_df.to_csv('./test_file_normalized.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading normalized testset\n",
      "Generating embeddings\n",
      "Embeddings generated\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c542078418524535a794f3df63de23bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Prediction:   0%|          | 0/6946 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class TorchDataset(Dataset):\n",
    "    def __init__(self, idxs, data):\n",
    "        super().__init__()\n",
    "        self.idxs = idxs\n",
    "        self.data = data\n",
    "        self.size = idxs.shape[0]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        users_main_data = self.data['user']\n",
    "        items_main_data = self.data['item']\n",
    "        \n",
    "        real_idxs = self.idxs[idx]\n",
    "        \n",
    "        users = users_main_data[real_idxs]\n",
    "        items = items_main_data[real_idxs]\n",
    "        \n",
    "        return users, items\n",
    "\n",
    "class TestsetEval:\n",
    "\n",
    "    def __init__(self, model_filepath) -> None:\n",
    "        # model = torch.load(model_filepath)\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model = torch.load(model_filepath, map_location=self.device)\n",
    "        model = model.to(self.device)\n",
    "        model.eval()\n",
    "        self.model = model\n",
    "\n",
    "    @staticmethod\n",
    "    def load_normalized_test_set():\n",
    "        test_df = pd.read_csv('./test_file_normalized.csv')\n",
    "        test_df.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "        return test_df\n",
    "    \n",
    "\n",
    "    def predict(self):\n",
    "        print('Loading normalized testset')\n",
    "        test_df = TestsetEval.load_normalized_test_set()\n",
    "        print('Generating embeddings')\n",
    "        dataset = data_maker.df_to_embeddings(test_df)\n",
    "        print('Embeddings generated')\n",
    "        test_dataset = TorchDataset(torch.arange(dataset['user'].size(0)), dataset)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=64, num_workers=1, shuffle=False)\n",
    "        with torch.no_grad():\n",
    "            self.model.eval()\n",
    "            outputs = []\n",
    "            probabilities_acc = []\n",
    "            for users, items in tqdm(test_loader, total=len(test_loader), desc=f'Prediction'):\n",
    "                users, items = users.to(self.device), items.to(self.device)\n",
    "                logits = self.model((users, items))\n",
    "                logits = self.model.softmax(logits)\n",
    "                probabilities = logits.cpu()\n",
    "                predictions = logits.argmax(dim=1).cpu()\n",
    "                outputs.append(predictions)\n",
    "                probabilities_acc.append(probabilities)\n",
    "            return torch.cat(outputs).numpy(), torch.cat(probabilities_acc).numpy()\n",
    "        \n",
    "testset_eval = TestsetEval('./models/torch-models-2024-02-16T17-36-50/model-final.pt')\n",
    "predictions, probabilities = testset_eval.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def report_submission():\n",
    "    pred = probabilities[:,1]\n",
    "    pred_df = pd.DataFrame(pred)\n",
    "    pred_df.reset_index(inplace=True)\n",
    "    pred_df.columns = ['Id','Predicted']\n",
    "    pred_df.to_csv('my_submission.csv',index=False)\n",
    "    return pd.read_csv('./my_submission.csv').head(15)\n",
    "report_submission()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
